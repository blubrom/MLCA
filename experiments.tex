% Created 2024-01-18 jeu. 16:17
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Clément Morand}
\date{\today}
\title{estimations from \cite{Strubell2019energy}}
\hypersetup{
 pdfauthor={Clément Morand},
 pdftitle={estimations from \cite{Strubell2019energy}},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.1.9)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\label{sec:strubell}
\section{Information about the hardware configuration}
\label{sec:orgd9accb7}

It is described in the paper that estimates are conducted by training
all models for a maximum of 24h. They use RAPL and NVIDIA System
Management Interface to measure the average consumption of the CPUs and
GPUs. 
All models are trained on one NVIDIA TITAN X except for ELMo
which is trained on 3 GTX 1080 Ti.
They then transcribe these results to estimates by using the training
time given in the paper and the description of the hardware given in
the paper.

No figures are presented regarding the average consumption of the
memory, CPU and GPU (separated). We only know about the model of GPU used for
estimating the consumption and the total estimated consumption for
training each model. We will therefore not give any value for
the CPU and ram and run our estimates as is. We will see what results
we obtain. We would like, not to obtain exact results since it wont be
possible given the information missing. Since they use measurement
tools, we can think that using a modeling using the \gls{TDP} will give
an higher result but since we do not know the quantity of memory used
and the CPU used, we are not sure that the results will be higher
(even if we can hypothesize that the CPU average consumption is
negligible compared to the GPU consumption.)

One reassuring point is that GTX 1080 Ti, V100, P100 and Titan X GPUs have the same
\gls{TDP} so the consumption estimated should make sense.

They use a \gls{PUE} of 1.58 and a Carbon Intensity of 0.954 pounds CO\(_{\text{2}}\)
e/kWh for American electricity production which is equivalent to
432.72 gCO\(_{\text{2}}\) e/kWh.

\section{Checking the Coherency of the presented results}
\label{sec:org445ce4e}

Since there are no estimates given for models trained on TPUs, we will
in the first time at least ignore these models.

Since table 3 of the paper presents the estimated consumption used, we can first
check the coherency of the table by seeing if we can reproduce the
same energy consumption by multiplying the power by the training time
and the \gls{PUE}

We can see that, up to rounding we obtain the same results.
We can also check that we obtain the same carbon emissions.

Also the same up to rounding errors. We can now serenely proceed with
running our estimations.

\section{running our estimations}
\label{sec:org67db772}

\subsection{approximating the GPU usage factor}
\label{sec:orga72c0d2}

Using the ratio of average measured power consumption to total TDP, we
can deduce an approximation of the GPU usage factor (assuming that the
vast majority of power draw comes from the GPUs)

\begin{center}
\begin{tabular}{lrr}
model & estimated & measured\\
\hline
Transformer\(_{\text{base}}\) & 2000 & 1415.78\\
Transformer\(_{\text{big}}\) & 2000 & 1515.43\\
ELMo & 750 & 517.66\\
BERT\(_{\text{base}}\) & 16000 & 12041.51\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{lr}
model & estimated GPU usage\\
\hline
Transformer\(_{\text{base}}\) & 0.70789\\
Transformer\(_{\text{big}}\) & 0.757715\\
ELMo & 0.6902133333333333\\
BERT\(_{\text{base}}\) & 0.752594375\\
\end{tabular}
\end{center}


\subsection{estimates}
\label{sec:org723ca1e}

\begin{table}[htbp]
\caption{Comparison of the presented measures with our estimates}
\centering
\begin{tabular}{lrrrrrrrrr}
model & expected & estimated & estimated & expected & estimated & estimated & expected & estimated & estimated\\
 & energy & energy match & energy base & CO2e & CO2e match & CO2e base & CO2e & CO2e match & CO2e base\\
 & (kWh) & (kWh) & (kWh) & (kg) & (kg) & (kg) & (lbs) & (lbs) & (lbs)\\
\hline
Transformer\(_{\text{base}}\) & 27 & 27 & 31 & 11.79 & 11 & 11 & 26 & 25 & 25\\
Transformer\(_{\text{big}}\) & 201 & 203 & 235 & 87.09 & 87 & 87 & 192 & 193 & 192\\
BERT\(_{\text{base}}\) & 1507 & 1500 & 1750 & 652.17 & 651 & 646 & 1438 & 1435 & 1424\\
NAS & 656347 & 871000 & 1.71\,(+06) & 284018 & 377000 & 632000 & 626155 & 831143 & 1.39332\,(+06)\\
ELMo & 275 & 404 & 793 & 118.84 & 175 & 293 & 262 & 385.81 & 645.95\\
\end{tabular}
\end{table}

Table \ref{tab:strubell_training} presents the results of our
estimates on two different scenarios. The first one (match) uses the
same \gls{PUE} and \gls{CI} as presented in the paper while the second
(base) uses the base values of our tool for the dynamic ratio and \gls{CI} of the USA. We can see that we obtain estimates that are, as expected, a little
bit higher than those presented.
We can explain the higher estimated energy when using the base values
for our tool because of the difference in Dynamic ratio. We use as
base value a dynamic ratio of 1.83 when the match scenario uses a
dynamic ratio of 1.58. We can also see that the estimated carbon
footprint is slightly higher in the match scenario than in the base
scenario ; this can be explained by the difference in \gls{CI}
used. Indeed, the \gls{CI} for the USA in the base values
is 370gCO\(_{\text{2}}\) e/kWh instead of the 432gCO\(_{\text{2}}\) e/kWh when trying to match.

\section{hyper-parameter search}
\label{sec:org78c3f28}

To complement the case study on hyper-parameter search and costs not
only on training one model but of the whole process, let us try and
reproduce similar results, which we would be able to study also in
terms of the other impacts estimated by our tool.

\begin{table}[htbp]
\caption{Comparison of the expected energy consumption and cost with our estimates}
\centering
\begin{tabular}{rrrrrr}
Models & Hours & Expected & Estimated & Expected & Estimated\\
 &  & energy & energy & electricity & electricity\\
 &  & (kWh) & (kWh) & cost (\$) & cost (\$)\\
\hline
1 & 120 & 41.7 & 55 & 5 & 7\\
24 & 2880 & 983 & 1320 & 118 & 158\\
4789 & 239942 & 82250 & 110000 & 9870 & 13200\\
\end{tabular}
\end{table}

Table \ref{tab:strubell_cost} compares the estimated energy consumption
and elecrity costs with the expected ones. We can see that we still obtain higher energy consumption  values thans
the ones presented. This fact can be mostly explained by the
difference between using a \gls{PUE} of 1.58 and a dynamic ratio of 1.83

\section{integrating Life cycle to previous analyses}
\label{sec:orgc82acf1}

If we now look at the full estimates produced by our tool and not only
on the direct impacts, 
We can see that the full impacts estimated for performing the whole
model search, hyper-parameter tuning and training represents the annual
impacts of 22 persons if we place ourselves in a scenario where we
would respect the "Stratégie Nationale Bas Carbone" for France
by 2050. If we place ourselves in the framework of the Planetary
boundaries, where if we want to stay sustainable, societies must not
overpass the planetary boundaries. The whole process accounts for the
maximal annual impacts of 44 persons in terms of Green House Gas
emissions and the annual impacts of 24 persons in terms of resource
depletion.

Of course, if computations were to be run in a country with a
less carbon intensive electricity mix, green warming potential would
be lower. Still, the impacts on resources depletion are very
important, and, in this estimation, we do not take into account any (1
GB) memory on the server that runs the experiments. 

If we were to add memory, for instance 512 GB of memory, we would
obtain the following estimation

with expected impacts as high as the maximal annual ones of 86 persons
in terms of \gls{GWP} and 33 persons in terms of Resources depletion
when not exceeding the planetary boundaries.

As a title of comparison, if we were to make the same estimates but
running in France, we would obtain the following (with a \gls{CI} of 98gCO\(_{\text{2}}\) e/kWh)

It would still represent the maximal annual emissions of 32 persons in
terms of \gls{GWP} or in terms of \gls{ADP}

This small change in \gls{ADP} impact can be explained by the fact that most of
the impacts on ressource depletion are due to manufacturing the
hardware used. This demonstrates the importance of both considering
embodied impacts and of considering other impacts than just the carbon footprint



\section{New exepriments with finer accounting of the servers used}
\label{sec:orgbaa1e59}

As it is really complex to find data regarding the hardware used in
TPUs, the experiments done with google's TPU will not be included

for the ELMo and Transformer case, the papers do not detail the
hardware. Just the graphics cards used are indicated in Caswani et. al
for the transformer models. 

For the memory use and CPU used for these two models:
\begin{itemize}
\item According to \href{https://docs.deeppavlov.ai/en/0.9.0/apiref/models/elmo.html}{this blogpost}, 32GB memory are required to train ELmo
\item According to \href{https://www.trentonbricken.com/TransformerMemoryRequirements/}{this blogpost}, 32GB memory should also work to train a
Transformer with 65M parameters and 64GB memory should work to train
a Transformer with 213M parameters
\item for the CPU, a CPU used in servers from the same period will be
used, specificaly, the CPU used in the Nvidia DGX-2H server, that
was used to train BERT in the experiments described in the Strubell
\textbf{et al.} paper: 2 Intel Xeon Platinum 8174
\end{itemize}

For the GPU usage factor, in order to not overestimate too much, a
value will be computed from the total consumption estimated by
Strubell and colleagues (supposing that the vast majority of
consumption comes from the GPU, the power consumption indicatged in
the paper are entirely attributed to the GPU).

For the remaining hardware, we will use the default values of MLCA.

\subsection{experiments setup}
\label{sec:org43f273c}

define the setup that will be used to run Transformer and ELMo
experiments.   

For Bert, it is stated in the paper that the estimates where run on
Nvidia 4 DGX-2H servers whose specifications are available at \href{https://www.nvidia.com/content/dam/en-zz/es\_em/Solutions/Data-Center/dgx-2/dgx-2h-datasheet-us-nvidia-841283-r6-web.pdf}{this
website}, each server comprising 2 Intel Xeon Platinum 8174 CPU, 2*
960GB NVME SSDs, 1.5TB memory 

create the different models for the different sensitivity analyses

define functions to run the estimates

\subsection{sensitivity}
\label{sec:orgd9f1b57}

To test the sensitivity of the results on the different parameters,
changes to the density of memory, lifespan of servers and usage ratio
will be tested. For the lifespan and usage ratio, we will produce an
uncertainty interval, as we suppose that these parameters are easily
bounded.
We suppose that no servers have a mean lifespan of less than 1 year
and no more than 8 years, and suppose that the servers never have a
usage ratio of less than 10\% and never have a higher ratio than 95\%.

Results will therefore be compared when using the default value and
the values producting the highest (lowest lifespan and usage) and lowest
(highest lifespan and usage) impacts.

For the sensitivity to changes in the estimated memory density, we wil
test to scenarios, one when using the default value and another one
when using the density used in \cite{Pirson2023iot} for instance (or
one of the values from \cite{Groger2021green}) 

Sensitivity to changes in location will be explored through two
different scenarios, one in France and one in the USA.

\subsection{running the estimates}
\label{sec:orgb1c6e0b}

\begin{center}
\begin{tabular}{llllllllll}
model & base & base & base & changed\(_{\text{density}}\) & changed\(_{\text{density}}\) & changed\(_{\text{density}}\) & changed\(_{\text{location}}\) & changed\(_{\text{location}}\) & changed\(_{\text{location}}\)\\
 & GWP & ADP & PE & GWP & ADP & PE & GWP & ADP & PE\\
 & (\COtwo{}) & (\Sbe{}) & (MJ) & (\COtwo{}) & (\Sbe{}) & (MJ) & (\COtwo{}) & (\Sbe{}) & (MJ)\\
\hline
Transformer\(_{\text{base}}\) & 12.0 [12.0,31.0] & 7.9e-05 [5.4e-05,0.0039] & 360.0 [360.0,590.0] & 12.0 [12.0,25.0] & 7.7e-05 [5.2e-05,0.0037] & 360.0 [360.0,520.0] & 3.5 [3.3,22.0] & 7.8e-05 [5.3e-05,0.0039] & 360.0 [360.0,580.0]\\
Transformer\(_{\text{big}}\) & 90.0 [89.0,220.0] & 0.00056 [0.00038,0.027] & 2700.0 [2700.0,4300.0] & 89.0 [88.0,180.0] & 0.00054 [0.00037,0.026] & 2700.0 [2700.0,3800.0] & 26.0 [25.0,160.0] & 0.00055 [0.00037,0.027] & 2700.0 [2700.0,4300.0]\\
BERT\(_{\text{base}}\) & 830.0 [810.0,3900.0] & 0.0049 [0.0034,0.24] & 24000.0 [24000.0,62000.0] & 800.0 [790.0,2100.0] & 0.0039 [0.0027,0.19] & 24000.0 [24000.0,40000.0] & 260.0 [240.0,3300.0] & 0.0048 [0.0033,0.24] & 24000.0 [24000.0,62000.0]\\
ELMo & 130.0 [120.0,440.0] & 0.0014 [0.00094,0.069] & 3800.0 [3800.0,7700.0] & 130.0 [120.0,380.0] & 0.0014 [0.00092,0.068] & 3800.0 [3800.0,7000.0] & 38.0 [36.0,350.0] & 0.0014 [0.00093,0.069] & 3800.0 [3700.0,7700.0]\\
\end{tabular}
\end{center}
\end{document}